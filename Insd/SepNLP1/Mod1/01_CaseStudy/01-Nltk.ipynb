{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Nltk.ipynb","provenance":[],"collapsed_sections":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hpT02vsTow48"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oRSxxSSV7PvD"},"source":["## Table of Contents\n","\n","1. [Introduction](#section1)<br>\n","2. [The basic Tasks in Natural Language Processing](#section4)<br>\n","    - 2.1 [Convert Text to Lower Text](#section401)<br>\n","    - 2.2 [Word Tokenize](#section402)<br>\n","    - 2.3 [Sent Tokenize](#section403)<br>\n","    - 2.4 [Stop words removal](#section404)<br>\n","    - 2.5 [Lemma](#section405)<br>\n","    - 2.6 [Stem](#section406)<br>\n","    - 2.7 [Get word frequency](#section407)<br>\n","    - 2.8 [Pos Tags](#section408)<br>\n","    - 2.9 [NER(Named Entity Recognition)](#section409)<br>\n","3. [Applications](#section408)<br>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L0Y-s377c2tS"},"source":["# Introduction\n","\n","### NLP(Natural Language Processing)\n","- Field of Computer Science and Linguistics: **interaction between computers and human languages.**\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/human-comp.jpg\"/></center>\n","\n","\n","\n","\n","- The main objective is to **enrich computer algorithms** with the capacity of handling natural language.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UsOQbFRrwXLN"},"source":["<center><img src = \" https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/comphuman.JPG\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ejglSvBAjrFE"},"source":["- Text often contain **slang, sarcasm, inconsistent syntax, grammatical errors, double entendres**, etc. \n","- Those create a problem for the machine algorithms as all **text mining techniques** require correctly **structured text sentences or documents**. \n","- So, if you still want to implement an algorithm, someone needs to correct all writing errors and **standardize word forms** to make them more comparable. \n","- The best way to do that is with **natural language processing tools**."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h7SFAtyOkC-F"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nlp222.JPG\" height = \"300\"></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iGxKw7Sr0eaB"},"source":["## What is the need for the usage of NLP libraries?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hN-TgSDm0cEn"},"source":["- These Libraries helps us to **extract meaning** from the **text** \n","- It includes the wide range of tasks such as **document classification, topic modelling, part-of-speech (POS) tagging, and sentiment analysis** etc."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AsKhraF8O5ik"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nlplibraries.JPG\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QI8G1Lqz0w5O"},"source":["## Let’s have a look at the following libraries:-\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ltk1.JPG\" height =\"80\"/></center>\n","\n","#### Natural Language Toolkit(NLTK)\n","- It is foremost platform for **constructing python programs** when we deal with linguistic data. \n","-  It is a suite of **libraries and programs for symbolic and statistical** natural language processing (NLP) for English written in the Python programming language.\n","- It is **open source, community-driven project.**\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/scikit.JPG\"/></center>\n","\n","#### Scikit-learn\n","- It is not just used for NLP but it’s applied widely in **Machine learning**.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/textblob.JPG\" height = \"200\"/></center>\n","\n","#### TextBlob\n","- It provides with **NLP tools API** which can be **easily handled** by the users.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/spacy.JPG\"/></center>\n","\n","#### spaCy\n","- Using spacy, we can implement concepts of NLP by using **Python and Cython**. \n","- It has some **excellent capabilities** for **named entity recognition**.\n","\n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/polyglot2.JPG\"/></center>\n","\n","#### Polyglot\n","- It is the yet **another python package** for NLP. \n","- It is **not very popular** but also can be used for a **wide range of the NLP tasks**.\n","- It is usually used for **projects involving a language** spaCy doesn’t support.\n","\n","\n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/gensim.JPG\" height = \"100\"/></center>\n","\n","#### Gensim\n","- It is specifically used for **Topic Modelling** while we deal with text documents.\n","\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stanford.JPG\"height = \"150\"/></center>\n","\n","\n","#### Stanford Core NLP\n","- One of the most famous university i.e. **Stanford University** has contributed a lot towards the **development of NLP.**\n","- There are numerous **NLP packages and services** which are built by Stanford NLP Group for the same purpose."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZZpysumToh56"},"source":["## Lets get started with Natural Language Processing \n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/1_tzWP1K-eXiehDBBUxUN11w.gif\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KTVfJrTI4GFP"},"source":["### The data obtained from different sources is full of noise i.e. errors which will affect the precision of the result.\n","\n","- So, we need to perform **cleaning** and **standardization** of the text. \n","- This will make the **text noise free** and **ready** for further analysis."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QeuJSWzWzv9M"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/strudata.JPG\" height = \"200\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TGiW2VpJGfSq"},"source":["## Forms of NLP data:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oUk7dd0DkDOI"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ictss.JPG\"/></center>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d9CFhpclA5UX"},"source":["## To make the data structured , we perform the following NLP tasks."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oV6QrrcCdvUr"},"source":["**The basic tasks in NLP are**:\n","\n","1. Convert text to lower case\n","2. Word tokenize\n","3. Sentence tokenize\n","4. Stop words removal\n","5. Lemmatization(Lemma)\n","6. Stemming(Stem)\n","7. Get word frequency\n","8. Part of Speech tags(POS)\n","9. Named Entity Recognition(NER)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rd9i1W7Qnp3Z"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/caps.JPG\"></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3FSw4G9QeFTC"},"source":["## Pre-requirements:\n","\n","- Install Python\n","- Install nltk and its corpus"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0k8nyKzEu2dg"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/cp1.JPG\" height = \"300\"/></center>\n","\n","### Corpus \n","- It is a **large collection of text.**\n","- The **Plural** of corpus is **Corpora.**\n","- It is written of spoken material on which **linguistic analysis** is used and is based upon.\n","- Its data is **stored** in **library installed** .\n","- **For Example** - Nltk library has following corpus : \n","  - Brown corpus\n","  - Gutenberg corpus\n","  - Shakespeare corpus\n","  - Stop words\n","  - Treebank corpus etc\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9ZmnMg8PMzS7"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/corpus2.JPG\"/></center>\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/2fw2xi.gif\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uYfbZNpPdjo_"},"source":["## Why Corpus?\n","- It provides **descriptive insights** relevant to how people **use language**.\n","- It acts as tool that enables to **analyse** both how people use **different language forms** at various levels of formality.\n","- It helps analyse how language fulfils **multiple speech functions** across contexts."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ixgt0spfJbGD"},"source":["### Lets install Brown corpus"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1019,"status":"ok","timestamp":1566815029151,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"p3ytsQS9Sou9","outputId":"f3f9c6af-9d65-4809-9f9c-2a4d64df6e85","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('brown')\n","import warnings\n","warnings.filterwarnings(action = 'ignore')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1061,"status":"ok","timestamp":1566814806126,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"GhG2q9beJpFX","outputId":"b5fbadbe-221f-4c21-80e9-aadccf6e703c","colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["from nltk.corpus import brown\n","brown.categories()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['adventure',\n"," 'belles_lettres',\n"," 'editorial',\n"," 'fiction',\n"," 'government',\n"," 'hobbies',\n"," 'humor',\n"," 'learned',\n"," 'lore',\n"," 'mystery',\n"," 'news',\n"," 'religion',\n"," 'reviews',\n"," 'romance',\n"," 'science_fiction']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eVWd2kRZoLZq"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/lady.jpg\" height = \"300\" /></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jU_Zq2WHoDv-"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/oni.JPG\" height = \"250\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gp6Z2soIePfN"},"source":["## Import nltk in order to use its functions."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bbHEtJkFefxb","colab":{}},"source":["import nltk\n","Br = nltk.corpus.brown.words() "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6IAeLVjTTo9H"},"source":["### Lets add words to the corpus "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sfx9YkoPSk-1","colab":{}},"source":["l = list(Br)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"djwxl2fSTzih"},"source":["### Let the word be 'talking'"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"m7D_aAP4SkqH","colab":{}},"source":["l.append('talking')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iWSzdBcBzJUT"},"source":["#### NLTK: Some Example Modules\n","\n","- **nltk.token**: processing individual elements of text, such as words or sentences.\n","\n","-  **nltk.probability**: modeling frequency distributions and probabilistic systems.\n","\n","-  **nltk.tagger**: tagging tokens with supplemental information, such as parts of speech or wordnet sense tags.\n","\n","- **nltk.parser**: high-level interface for parsing texts.\n","\n","-  **nltk.chartparser**: a chart-based implementation of the parser interface.\n","\n","- **nltk.chunkparser**: a regular-expression based surface parser."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6XfQFdJBeoK1"},"source":["## Coverting text to lower text as it is case sensitive"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PF8uuumNVvuB"},"source":["<center><img src= \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/wrong.JPG\" height = \"300\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1082,"status":"ok","timestamp":1566814829221,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"kZEkdTpneqFI","outputId":"8d15d452-91e9-4f3a-fdf4-d539a50fe045","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text =  \"Full form of NLTK is Natural Language Toolkit\"\n","lower_text = text.lower()\n","print (lower_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["full form of nltk is natural language toolkit\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wymdpTnnWKFE"},"source":["## Tokenisation\n","\n","- The task of converting a text from a **single string** to a **list of tokens** is known as tokenization.\n","- The tokens may be **words** or **number** or **punctuation mark**. \n","   - Word tokenisation.\n","   - Sentence tokenisation\n","   \n"," <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/tokenis.JPG\"/></center>\n"," \n"," \n"," ## Why tokenisation?\n"," - Computers cannot understand **large chunks** of data.\n"," - As a part of **preprocessing**, the text is broken into pieces called **Tokens**.\n"," - This is important because the **meaning of text** generally depends on the **relations of words** in that text.\n"," - These Tokens are **fed as an input** for other types of **analysis or tasks**.\n","  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"25OFKHH-ewVA"},"source":["## Word tokenize\n","- Tokenize sentences to get the **tokens** of the **text** i.e **breaking the sentences** into words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n4NqLmp_e5zN","colab":{}},"source":["import nltk\n","from nltk import sent_tokenize, word_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RXgG68U78X9l"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/891.JPG\"/></center>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1421,"status":"ok","timestamp":1566814843094,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"29tM5HLefGQf","outputId":"49a18927-0f33-4113-96a2-ab833895451f","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KbNLaEUbB1u9"},"source":["#### Lets create tokens of a sentence "]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1025,"status":"ok","timestamp":1566814846457,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"Y4txZ2iGe98C","outputId":"3dbb59f9-5b85-4988-e4af-58a5c1f2bb81","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text = \"Full form of NLTK is Natural Language Toolkit\"\n","word_tokens = nltk.word_tokenize(text)\n","print (word_tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Full', 'form', 'of', 'NLTK', 'is', 'Natural', 'Language', 'Toolkit']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"urv8ij73fM1U"},"source":["## Sent tokenize\n","- Tokenize sentences if there are **more than 1 sentence** i.e **breaking** the **sentences** to **list of sentence**."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1008,"status":"ok","timestamp":1566814871685,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"6BeErMnBfSJY","outputId":"d2222e13-7836-4835-99ae-9c344ee63b1c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text = \"Full form of NLTK is Natural Language Toolkit. It is a very powerful Package.\"\n","sent_token = nltk.sent_tokenize(text)\n","print (sent_token)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Full form of NLTK is Natural Language Toolkit.', 'It is a very powerful Package.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zM73kGIZfZNT"},"source":["## Stop words removal\n","- Remove **irrelevant words** using **nltk stop words** like is,the,a etc from the sentences as they **don’t carry any information**."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ybYUySG4kua8"},"source":["<center> <img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stopwords.JPG\"/> </center>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3HdcCYnLfckx","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":979,"status":"ok","timestamp":1566814879851,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"i3xxBAWSfr5g","outputId":"4c2aeb3d-c7e8-42f5-d1b0-f78127415768","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["nltk.download('stopwords')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pK-AvxPooerW"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/stop%20words.JPG\" height = \"300\"></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1004,"status":"ok","timestamp":1566814884196,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"rAGjxjWfffVe","outputId":"a3027e7d-c2b0-4276-94d6-a198bf780774","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stopword = stopwords.words(\"english\")\n","text =  \"Full form of NLTK is Natural Language Toolkit\"\n","word_tokens = nltk.word_tokenize(text)\n","removing_stopwords = [word for word in word_tokens if word not in stopword]\n","print (removing_stopwords)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Full', 'form', 'NLTK', 'Natural', 'Language', 'Toolkit']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gINKqoZOfzTR"},"source":["## Lemmatization\n","- **Lemmatize the text** so as to get its **root form** \n","- For Example: **functions,funtionality as function**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Nz5wmF1IoFou"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/89.JPG\" height = \"200\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GXqH7plUf09_","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","#is based on The Porter Stemming Algorithm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1078,"status":"ok","timestamp":1566814894785,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"iGXf7ZovgRZ-","outputId":"cf834307-dd14-47af-f15a-5e81233d8175","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["nltk.download('wordnet')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":2946,"status":"ok","timestamp":1566814899169,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"N1DEos4pf91g","outputId":"c9878341-a4a6-4dbb-8876-6c3936a42503","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stopword = stopwords.words(\"english\")\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"How are your studies going on? \"\n","word_tokens = nltk.word_tokenize(text)\n","lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n","print (lemmatized_word)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['How', 'are', 'your', 'study', 'going', 'on', '?']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8w4F6DecgX0Q"},"source":["## Stemming\n","- It is the process of **reducing inflected** (or sometimes derived) words to their word stem, **base or root form**."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nRGKDx50n61M"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/nant.JPG\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"biajxK-BgZFg","colab":{}},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","#is based on The Porter Stemming Algorithm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1026,"status":"ok","timestamp":1566814906013,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"UXKiFzlDf_W_","outputId":"51246b44-7fda-484c-ef71-b704f804426f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stopword = stopwords.words(\"english\")\n","snowball_stemmer = SnowballStemmer(\"english\")\n","text = \"How are your studies going on?\"\n","word_tokens = nltk.word_tokenize(text)\n","stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n","print (stemmed_word)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['how', 'are', 'your', 'studi', 'go', 'on', '?']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vULmjhrivaIz"},"source":["## Lemmatization Vs Stemming?\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/superman-vs-flash-justice-league.png\" height = \"200\"/></center>\n","\n","- Lemmatization is typically **more accurate** as it uses more informed analysis to create **groups of words** with **similar meaning** based on the content around the world.\n","- While Stemming is **typically faster** as it chops off the end of a word **using heuristics**, **without** any **understanding** of the context in which a word is used apparently.\n","- By applying Stemming we **may or may not** get a **meaningful word.**\n","- By using Lemmatization the words of a sentence will give a **meaningful meaning** of word.\n","- Lemmatization is **accurate** but is **computationally expensive**\n","- Stemming is **not that accurate** but it is **faster.**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3r7A7w6Ygu0X"},"source":["## Get word frequency\n","- Through this we convert the **text into vector models** which is based on the occurrence of words in document\n","- Counting the **word occurrence** using **FreqDist library**."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1298,"status":"ok","timestamp":1566814915169,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"etuWF5lUgxlA","outputId":"47bdd274-7fb0-48c2-b37a-155ca53a4df3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import nltk\n","from nltk import FreqDist\n","text = \"Full form of NLTK is Natural Language Toolkit\"\n","word = nltk.word_tokenize(text.lower())\n","freq = FreqDist(word)\n","print (freq.most_common(5))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('full', 1), ('form', 1), ('of', 1), ('nltk', 1), ('is', 1)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZBeGtmnBt8zD"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/frequencyhighwords.jpg\" height = \"200\"></center>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xMLbnrN5g8Rf"},"source":["##  POS(Part of Speech)tags\n","- It helps us to know the tags of **each word** like whether a **word is noun, adjective** etc."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I-iUddF_0grR"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/ner.JPG\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1021,"status":"ok","timestamp":1566814924214,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"1cbvMXc9htyF","outputId":"bd7bf19e-418d-4c81-98a3-44945dee398f","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1040,"status":"ok","timestamp":1566814926994,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"4MRb5-FvhdVC","outputId":"24e33be4-4b8c-40eb-ca3a-1535b8a893ae","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import nltk\n","text = \"the dogs are barking outside.\"\n","word = nltk.word_tokenize(text)\n","pos_tag = nltk.pos_tag(word)\n","print (pos_tag)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GEQ1LZoAbeN8"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/partofspeechtags.JPG\" height = \"300\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zt-vX6ulhJjQ"},"source":["##  NER(Named Entity Recognition)\n","- It is the process of getting the **entity names**.\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/entity2.JPG\" height = \"400\"/></center>\n","- It is the subtask of **information extraction** that classify named entities into predefined categories such as **name of person, organization, location** etc."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1400,"status":"ok","timestamp":1566814932250,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"EKfc9ykhh2Il","outputId":"a88875ab-84d3-4a13-e274-5b5daff4a364","colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SQXndvSlujGo"},"source":["<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/named.JPG\" height = \"280\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1022,"status":"ok","timestamp":1566814935942,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"F-UV0Z_gijE4","outputId":"089dc12d-4300-42db-ea00-87f172bef34e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["text = \"Narendar Modi is the Prime Minister of India. Ananta is working at Insaid on Datascience pipelines\"\n","word = nltk.word_tokenize(text)\n","pos_tag = nltk.pos_tag(word)\n","chunk = nltk.ne_chunk(pos_tag)\n","NE = [ \" \".join(w for w, t in ele) for ele in chunk if isinstance(ele, nltk.Tree)]\n","print (NE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Narendar', 'Modi', 'India', 'Ananta', 'Insaid', 'Datascience']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CKkrIPyHgoG1"},"source":["## Applications:\n","- We can use NLP on a text review to predict if the **review is a good one** or a **bad one.**\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/sentiment%20analysis.gif\"></center>\n","\n","- It can be used  on an **article to predict** some categories of the articles you are trying to segment.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/art.png\"></center>\n","\n","\n","- It can be used for certain criteria on which it takes **decision** and **blocks spam**(unwanted mails).<br>\n","<br>\n","\n","\n","\n","\n","<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/spam%20filter.gif\" height = \"150\"></center><br>\n","<br>\n","\n","\n","-  It can be used as a **chat Bot** that uses AI as **integral part** of business world.<br><br>\n","\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/4.gif\" height = \"200\"></center>"]}]}